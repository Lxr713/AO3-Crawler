# AO3 Fanfiction Crawler

A lightweight, dependency-free crawler for [Archive of Our Own (AO3)](https://archiveofourown.org/), written in pure Python. Extracts fanfiction metadata and chapter content with minimal setup.

## Features

- üöÄ **Single Article Crawling** - Extract title, author, and all chapters
- üìñ **Multi-chapter Support** - Handles works with any number of chapters
- üîç **Search Page Extraction** - Build work_id lists from search/tag pages
- üì¶ **Batch Crawling** - Process multiple works with built-in rate limiting
- üéØ **No External Dependencies** - Pure Python standard library (basic scripts)
- ‚ö° **Optimized Async Version** - Concurrent crawling with `aiohttp` (see `extract_optimized.py`)
- üíæ **Checkpoint/Resume** - Automatically resume interrupted crawls
- üõ°Ô∏è **Robust Error Handling** - Exponential backoff for HTTP 525/429/timeout errors
- üìù **Structured Logging** - Standard Python `logging` with file and console output
- ‚è±Ô∏è  **Polite Crawling** - Configurable delays to respect server load

## Requirements

- Python 3.6+

No external packages required!

## Quick Start

### 1. Single Article

```bash
python3 fetch.py 79906886
```

Output: `ao3_79906886.json`

### 2. Batch Crawling (Standard)

```bash
# Step 1: Extract work_ids from a search page
python3 extract.py

# Step 2: Batch crawl all works
python3 batch.py
```

### 3. Optimized Extraction (Recommended for Large Batches)

For crawling 1000+ pages with automatic resume, concurrent requests, and robust error handling:

```bash
# Install dependencies
pip install aiohttp aiofiles

# Run optimized extractor
python3 extract_optimized.py
```

Features:
- **Async concurrent crawling** (3 pages at a time)
- **Automatic checkpoint/resume** - interrupt and resume anytime
- **Exponential backoff** for HTTP 525/429/timeout errors
- **Structured logging** to console and `crawler.log`
- **Signal handling** - graceful shutdown on Ctrl+C

### 4. Optimized Batch Crawling (Recommended)

For large-scale batch crawling with checkpoint/resume, concurrent requests, and robust error handling:

```bash
# First, extract work_ids using optimized extractor
python3 extract_optimized.py

# Then, run optimized batch crawler (reads from checkpoint.json)
python3 batch_optimized.py
```

**Input Source**: `checkpoint.json` (generated by `extract_optimized.py`)

**Features**:
- **Reads from checkpoint.json** - Uses work_ids from `extract_optimized.py` output
- **Async concurrent crawling** - 3 works at a time (configurable)
- **Automatic checkpoint/resume** - Saves progress to `batch_checkpoint.json`
- **Exponential backoff** - Handles HTTP 525/429/timeout errors
- **Structured logging** - Logs to console and `batch_crawler.log`

## Usage

### Single Article Crawling

```bash
python3 fetch.py <work_id>
```

**Example:**

```bash
python3 fetch.py 79906886
python3 fetch.py https://archiveofourown.org/works/79906886
```

**Output Format:**

```json
{
  "work_id": "79906886",
  "url": "https://archiveofourown.org/works/79906886",
  "title": "„ÄêÁãÇËÅ™„Äë‰∏ÄËµ∑‰ΩèÂêß",
  "author": "Turmali",
  "total_chapters": 1,
  "chapters_fetched": 1,
  "chapters": [
    {
      "chapter_id": "1",
      "chapter_title": "Chapter 1",
      "content": "Full chapter text here..."
    }
  ]
}
```

### Extract Work IDs from Search Pages (Standard)

Edit `extract.py` to customize the search URL:

```python
# Default: All Chinese language works
search_url = 'https://archiveofourown.org/works/search?work_search[language_id]=zh&commit=Search'

# Example: Specific fandom
search_url = 'https://archiveofourown.org/tags/Rick%20and%20Morty/works'

# Example: Completed works, Chinese only
search_url = 'https://archiveofourown.org/works/search?work_search[complete]=1&work_search[language_id]=zh&commit=Search'
```

Run the extractor:

```bash
python3 extract.py
```

Output: `work_ids.txt` (one work_id per line)

### Batch Crawling (Standard)

```bash
python3 batch.py
```

The script:
- Reads work_ids from `work_ids.txt`
- Crawls each work with a 3-second delay
- Saves each work as `ao3_{work_id}.json`
- Generates `batch_summary.json` with statistics

**Adjust delay:** Edit `batch.py`:

```python
results = batch_fetch(work_ids, delay=3)  # 3 seconds between requests
```

## Scripts

| Script | Purpose | Dependencies |
|--------|---------|--------------|
| `fetch.py` | Single article crawler | Pure Python (stdlib only) |
| `extract.py` | Extract work_ids from search pages | Pure Python (stdlib only) |
| `batch.py` | Batch crawl multiple works | Pure Python (stdlib only) |
| `extract_optimized.py` | **Optimized** async work_id extractor | `aiohttp`, `aiofiles` |
| `batch_optimized.py` | **Optimized** async batch crawler | `aiohttp`, `aiofiles` |

## Technical Details

### view_full_work=true Optimization

Uses AO3's `?view_full_work=true` parameter to fetch all chapters in a single request, eliminating the need for pagination logic.

### HTML Parsing

Uses regex-based parsing (pure Python standard library) to extract:
- Title and author metadata
- Chapter headings and content
- Multi-chapter work structure

### Rate Limiting

Built-in delays protect both:
- AO3 server load (respectful crawling)
- Your network connection

## Testing

| Test Case | Status | Work ID |
|-----------|--------|---------|
| Single-chapter work | ‚úÖ | 79906886 |
| Multi-chapter work | ‚úÖ | 79779056 (2 chapters) |
| Work ID extraction | ‚úÖ | Chinese search page |
| Batch crawling | ‚úÖ | Ready to use |

## Limitations

- **Rate Limiting**: AO3 may throttle aggressive crawlers. Use reasonable delays.
- **HTML Structure**: Changes to AO3's HTML may break the parser.
- **Authentication**: Does not handle logged-in content (only public works).

## Disclaimer

This tool is for educational and personal use only. Please:

1. **Respect AO3's Terms of Service**
2. **Use reasonable crawl rates** (avoid overwhelming the server)
3. **Crawl responsibly** - consider using official APIs if available
4. **Attribute properly** if using data for publications

AO3 is a volunteer-run project. Be kind to their servers!

## Contributing

Issues and pull requests are welcome. Areas for improvement:

- [x] Handle HTML structure changes gracefully
- [x] Add retry logic for failed requests (exponential backoff in optimized versions)
- [x] Support resume/checkpoint for large batches (implemented in optimized versions)
- [ ] Export to CSV/SQLite formats
- [ ] Add more metadata fields (tags, kudos, bookmarks)

## License

MIT License - feel free to use, modify, and distribute.

## Acknowledgments

Built for research purposes to study fanfiction content patterns. Respecting the creative work of both AO3 volunteers and fanfiction authors.
